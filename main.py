# main.py
import json
import logging
import os
import re
from contextlib import asynccontextmanager
from dataclasses import dataclass
from itertools import product
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import uvicorn
from fastapi import Depends, FastAPI, HTTPException, Request
from pydantic import BaseModel

# =======================
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
# =======================

KW_SUBP = r"(?:–ø–æ–¥–ø—É–Ω–∫—Ç\w*|–ø–æ–¥–ø–ø\.|–ø–æ–¥–ø\.|–ø–ø\.)"

# –í–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ-—Å–∞–º–æ—Ç–µ—Å—Ç—ã –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞
RUN_STARTUP_SELFTESTS = True

LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=LOG_LEVEL,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
)
logger = logging.getLogger("law-links-service")

# ==========================
# Pydantic-–º–æ–¥–µ–ª–∏ –æ—Ç–≤–µ—Ç–∞ API
# ==========================

class LawLink(BaseModel):
    law_id: Optional[int] = None
    article: Optional[str] = None
    point_article: Optional[str] = None
    subpoint_article: Optional[str] = None


class LinksResponse(BaseModel):
    links: List[LawLink]


class TextRequest(BaseModel):
    text: str


# ======================
# –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
# ======================

@dataclass(frozen=True)
class ParsedRef:
    law_id: int
    article: Optional[str]
    point: Optional[str]
    subpoint: Optional[str]


# =================================
# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ-–∏–∏
# =================================

QUOTE_CHARS = {
    "¬´": '"', "¬ª": '"',
    "‚Äú": '"', "‚Äù": '"',
    "‚Äû": '"', "‚Äü": '"',
    "‚Ä≤": "'", "‚Äö": '"',
}
DASH_CHARS = {"‚Äì": "-", "‚Äî": "-", "-": "-"}

L2C_MAP = str.maketrans({
    # –ª–∞—Ç–∏–Ω—Å–∫–∏–µ -> –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ (look-alikes)
    "A": "–ê", "a": "–∞",
    "B": "–í", "b": "–≤",
    "E": "–ï", "e": "–µ",
    "K": "–ö", "k": "–∫",
    "M": "–ú", "m": "–º",
    "H": "–ù", "h": "–Ω",
    "O": "–û", "o": "–æ",
    "P": "–†", "p": "—Ä",
    "C": "–°", "c": "—Å",
    "T": "–¢", "t": "—Ç",
    "X": "–•", "x": "—Ö",
    "Y": "–£", "y": "—É",
})

def normalize_text(s: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∫–∞–≤—ã—á–∫–∏/–¥–µ—Ñ–∏—Å—ã/–ø—Ä–æ–±–µ–ª—ã (–±–µ–∑ –ª–∞—Ç–∏–Ω–∏—Ü–∞‚Üí–∫–∏—Ä–∏–ª–ª–∏—Ü–∞!)."""
    for a, b in QUOTE_CHARS.items():
        s = s.replace(a, b)
    for a, b in DASH_CHARS.items():
        s = s.replace(a, b)
    s = re.sub(r"[ \t]{2,}", " ", s)
    return s
L2C_MAP = str.maketrans({
    # –ª–∞—Ç–∏–Ω—Å–∫–∏–µ ‚Üí –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ (look-alikes) ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π –∑–∞–∫–æ–Ω–æ–≤
    "A": "–ê", "a": "–∞",
    "B": "–í", "b": "–≤",
    "E": "–ï", "e": "–µ",
    "K": "–ö", "k": "–∫",
    "M": "–ú", "m": "–º",
    "H": "–ù", "h": "–Ω",
    "O": "–û", "o": "–æ",
    "P": "–†", "p": "—Ä",
    "C": "–°", "c": "—Å",
    "T": "–¢", "t": "—Ç",
    "X": "–•", "x": "—Ö",
    "Y": "–£", "y": "—É",
})

def normalize_for_alias(s: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫ –∏–º–µ–Ω–Ω–æ –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –∑–∞–∫–æ–Ω–æ–≤."""
    s = normalize_text(s)
    s = s.translate(L2C_MAP) 
    s = s.strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s

def norm_alias_key(s: str) -> str:
    return normalize_for_alias(s)


def build_alias_maps(codex_aliases: Dict[str, List[str]]):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - alias_to_id: {–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∞–ª–∏–∞—Å -> law_id}
      - LAW_NAMED:    –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π regex —Å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –≥—Ä—É–ø–ø–∞–º–∏ (?P<LID_15>...)
      - LAW_NONCAP:   —Ç–æ—Ç –∂–µ regex, –Ω–æ –±–µ–∑ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø (?:...)
      - lid_group_names: {'LID_15': 15, ...}
    """
    alias_to_id: Dict[str, int] = {}
    lid_group_names: Dict[str, int] = {}
    parts: List[str] = []

    LOOKALIKE = {
        "–ê":"A","–í":"B","–ï":"E","–ö":"K","–ú":"M","–ù":"H","–û":"O","–†":"P","–°":"C","–¢":"T","–•":"X","–£":"Y",
        "–∞":"a","–≤":"b","–µ":"e","–∫":"k","–º":"m","–Ω":"h","–æ":"o","—Ä":"p","—Å":"c","—Ç":"t","—Ö":"x","—É":"y",
    }

    def flex_char(ch: str) -> str:
        return f"[{re.escape(ch)}{LOOKALIKE[ch]}]" if ch in LOOKALIKE else re.escape(ch)

    UPPER_RU = r"[–ê-–Ø–Å]"
    LOWER_RU = r"[–∞-—è—ë]"
    RU_WORD   = r"[–ê-–Ø–∞-—è–Å—ë]+"

    def is_all_caps_short(tok: str) -> bool:
        # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –Ω–∞–ø–æ–¥–æ–±–∏–µ "–ù–ö", "–£–ö", "–ì–ü–ö", "–ö–æ–ê–ü" (–ø–æ—Å–ª–µ–¥–Ω–µ–µ –Ω–µ all-caps, –Ω–æ –∫–æ—Ä–æ—Ç–∫–æ–µ)
        return bool(re.fullmatch(rf"{UPPER_RU}{{2,5}}", tok))

    def flex_word(tok: str) -> str:
        """
        –ì–∏–±–∫–æ–µ —Å–ª–æ–≤–æ:
          - '–†–§' –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ —Å LOOKALIKE
          - –ö–æ—Ä–æ—Ç–∫–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (–ì–ü–ö, –ù–ö...) –±–µ–∑ —Ö–≤–æ—Å—Ç–∞ '[–∞-—è—ë]*'
          - –î–ª—è –æ–±—ã—á–Ω—ã—Ö —Å–ª–æ–≤ ‚Äî –¥–æ–ø—É—Å–∫–∞–µ–º –º–æ—Ä—Ñ. –æ–∫–æ–Ω—á–∞–Ω–∏—è —á–µ—Ä–µ–∑ '[–∞-—è—ë]*'
          - –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞ -—ã–π/-–∏–π/-–æ–π 
        """
        if tok.upper() == "–†–§":
            return "".join(flex_char(c) for c in tok)

        if is_all_caps_short(tok):
            return "".join(flex_char(c) for c in tok)

        if re.search(r"(—ã–π|–∏–π|–æ–π)$", tok, flags=re.IGNORECASE):
            stem = tok[:-2]
            stem = "".join(flex_char(c) for c in stem)
            return fr"{stem}{LOWER_RU}+"

        stem = "".join(flex_char(c) for c in tok)
        return fr"{stem}{LOWER_RU}*"

    def alias_to_pattern(alias: str) -> str:
        alias = normalize_text(alias)
        tokens = re.split(r"(\s+)", alias)
        out = []
        for t in tokens:
            if t.isspace():
                out.append(r"\s+")
            elif re.fullmatch(RU_WORD, t) and len(t) >= 2:
                out.append(flex_word(t))
            else:
                out.append(re.escape(t))
        core = "".join(out)
        # –ñ—ë—Å—Ç–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤–∞ –≤–æ–∫—Ä—É–≥ –í–ï–°–¨ –∞–ª–∏–∞—Å–∞, —á—Ç–æ–± –Ω–µ –º–∞—Ç—á–∏—Ç—å –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤
        return rf"(?<![0-9A-Za-z–ê-–Ø–∞-—è–Å—ë])(?:{core})(?![0-9A-Za-z–ê-–Ø–∞-—è–Å—ë])"

    # –¥–ª–∏–Ω–Ω—ã–µ –∞–ª–∏–∞—Å—ã ‚Äî –ø–µ—Ä–≤—ã–º–∏
    all_items = []
    for lid_str, aliases in codex_aliases.items():
        lid = int(lid_str)
        for a in aliases:
            all_items.append((lid, a))
    all_items.sort(key=lambda x: len(x[1]), reverse=True)

    by_id: Dict[int, List[str]] = {}
    for lid, alias in all_items:
        alias_to_id[normalize_for_alias(alias)] = lid
        by_id.setdefault(lid, []).append(alias_to_pattern(alias))

    for lid, patts in by_id.items():
        gname = f"LID_{lid}"
        lid_group_names[gname] = lid
        parts.append(fr"(?P<{gname}>" + "|".join(patts) + ")")

    LAW_NAMED = "(?:" + "|".join(parts) + ")"
    LAW_NONCAP = re.sub(r"\(\?P<LID_\d+>", "(?:", LAW_NAMED)
    return alias_to_id, LAW_NAMED, LAW_NONCAP, lid_group_names



# ============
# –†–∞–∑–±–æ—Ä –∑–Ω–∞—á–µ–Ω–∏–π
# ============

# –û–¥–∏–Ω "–∞—Ç–æ–º" –∑–Ω–∞—á–µ–Ω–∏—è: 3, 3.1, 3.4.5, –±—É–∫–≤–∞ (ru/en), –¥–æ–ø—É—Å–∫–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –≤–æ–∫—Ä—É–≥
ATOM = r"(?:\d+(?:\.\d+)*|[¬´¬ª\"'‚Äú‚Äù‚Äò‚Äô]?[A-Za-z–ê-–Ø–∞-—è—ë][¬ª¬´\"'‚Äú‚Äù‚Äò‚Äô]?)"

# –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è: –∑–∞–ø—è—Ç–∞—è, —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π, "–∏", "–∏–ª–∏", "–∏/–∏–ª–∏", "–ª–∏–±–æ", –¥–µ—Ñ–∏—Å –¥–ª—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞
VAL_CHUNK = fr"{ATOM}(?:\s*(?:,\s*|;\s*|–∏\s+|–∏–ª–∏\s+|–∏\/–∏–ª–∏\s+|–ª–∏–±–æ\s+|-)\s*{ATOM})*"

def _is_letter(s: str) -> bool:
    return bool(re.fullmatch(r"[A-Za-z–ê-–Ø–∞-—è—ë]", s))

def _expand_letter_range(a: str, b: str) -> List[str]:
    # –î–∏–∞–ø–∞–∑–æ–Ω –±—É–∫–≤: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É –∏ –ª–∞—Ç–∏–Ω–∏—Ü—É
    a0, b0 = a.lower(), b.lower()
    if not (_is_letter(a0) and _is_letter(b0)):
        return [a, b]

    # –í—ã–±–µ—Ä–µ–º –∞–ª—Ñ–∞–≤–∏—Ç –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ
    def is_lat(ch: str) -> bool:
        return bool(re.fullmatch(r"[a-z]", ch))

    def alphabet(ch: str) -> List[str]:
        if is_lat(ch):
            return [chr(c) for c in range(ord('a'), ord('z')+1)]
        else:
            # —Ä—É—Å—Å–∫–∞—è –∞–∑–±—É–∫–∞ –±–µ–∑ '—ë' –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ; –¥–æ–±–∞–≤–∏–º '—ë' –≤ –Ω—É–∂–Ω–æ–µ –º–µ—Å—Ç–æ
            # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –≤–æ–∑—å–º—ë–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –≤ Unicode –æ—Ç '–∞' –¥–æ '—è' –∏ –æ—Ç–¥–µ–ª—å–Ω–æ —É—á—Ç—ë–º '—ë'
            rus = [chr(c) for c in range(ord('–∞'), ord('—è')+1)]
            # –∏–Ω–æ–≥–¥–∞ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è '—ë' ‚Äî –¥–æ–±–∞–≤–∏–º –µ—ë –≤ –∞–ª—Ñ–∞–≤–∏—Ç (–ø–æ—Å—Ç–∞–≤–∏–º –ø–æ—Å–ª–µ '–µ')
            if '—ë' not in rus:
                rus.insert(rus.index('–µ')+1, '—ë')
            return rus

    if is_lat(a0) != is_lat(b0):
        # —Ä–∞–∑–Ω—ã–µ –∞–ª—Ñ–∞–≤–∏—Ç—ã ‚Äî –Ω–µ —Ä–∞—Å—à–∏—Ä—è–µ–º
        return [a, b]

    alpha = alphabet(a0)
    try:
        ia, ib = alpha.index(a0), alpha.index(b0)
    except ValueError:
        return [a, b]

    if ib < ia:
        ia, ib = ib, ia
    return alpha[ia:ib+1]



def _expand_numeric_range(a: str, b: str) -> List[str]:
    # –ü–æ–¥–¥–µ—Ä–∂–∫–∞:
    #  "1-3" -> 1,2,3
    #  "43.2-6" -> 43.2,43.3,43.4,43.5,43.6
    #  "1.1-1.3" -> 1.1,1.2,1.3
    def parse_levels(x: str) -> List[int]:
        return [int(t) for t in x.split(".")]

    def join_levels(levels: List[int]) -> str:
        return ".".join(str(t) for t in levels)

    if _is_letter(a) or _is_letter(b):
        return [a, b]

    a_levels = parse_levels(a)
    b_levels = parse_levels(b) if "." in b else None

    # –°–ª—É—á–∞–π —Ç–∏–ø–∞ 43.2-6
    if b_levels is None and "." in a:
        prefix = a_levels[:-1]
        a_last = a_levels[-1]
        b_last = int(b)
        if b_last < a_last:
            a_last, b_last = b_last, a_last
        return [join_levels(prefix + [k]) for k in range(a_last, b_last + 1)]

    # –°–ª—É—á–∞–π 1-3
    if b_levels is None and "." not in a:
        a0, b0 = int(a), int(b)
        if b0 < a0:
            a0, b0 = b0, a0
        return [str(k) for k in range(a0, b0 + 1)]

    # –°–ª—É—á–∞–π 1.1-1.3 (–∏–ª–∏ 3.4.1-3.4.5) ‚Äì –≤–∞—Ä—å–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É
    if b_levels is not None and len(a_levels) == len(b_levels) and a_levels[:-1] == b_levels[:-1]:
        start, end = a_levels[-1], b_levels[-1]
        if end < start:
            start, end = end, start
        prefix = a_levels[:-1]
        return [join_levels(prefix + [k]) for k in range(start, end + 1)]

    # –ò–Ω–∞—á–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
    return [a, b]



def _split_by_commas_and_conj(s: str) -> List[str]:
    """
    –†–µ–∂–µ–º –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ , ; –∏ —Å–æ—é–∑–∞–º '–∏/–∏–ª–∏', '–∏–ª–∏', '–ª–∏–±–æ', '–∏'.
    –í–ê–ñ–ù–û: –µ—Å–ª–∏ chunk ‚Äî —ç—Ç–æ –û–î–ù–ê –±—É–∫–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–∏'), —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º,
    –∞ –Ω–µ —Å–æ—é–∑–æ–º.
    """
    s = s.strip()
    if not s:
        return []
    # –æ–¥–∏–Ω–æ—á–Ω–∞—è –±—É–∫–≤–∞ ‚Äî —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ, –Ω–µ —Å–æ—é–∑
    if re.fullmatch(r"[A-Za-z–ê-–Ø–∞-—è—ë]", s):
        return [s]
    # –æ–±—ã—á–Ω—ã–π —Å–ª—É—á–∞–π ‚Äî —Ä–µ–∂–µ–º –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º/—Å–æ—é–∑–∞–º
    parts = re.split(r"\s*(?:,|;|–∏\/–∏–ª–∏|–ª–∏–±–æ|–∏–ª–∏|–∏)\s*", s, flags=re.IGNORECASE)
    return [p for p in parts if p]


def parse_values(chunk: Optional[str], *, expand_hyphens: bool = True) -> List[str]:
    """
    –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç '1, 2 –∏ 3', '–∞-–≤', '43.2-6' –≤ —Å–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π.
    –ï—Å–ª–∏ expand_hyphens=False (–¥–ª—è —Å—Ç–∞—Ç–µ–π), –ù–ï —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω, –æ—Å—Ç–∞–≤–ª—è–µ–º
    —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –µ—Å—Ç—å ('51.8-7').
    """
    if not chunk:
        return []
    parts = _split_by_commas_and_conj(chunk)
    out: List[str] = []
    for p in parts:
        p = p.strip()
        if "-" in p and expand_hyphens:
            a, b = [q.strip() for q in p.split("-", 1)]
            if _is_letter(a) and _is_letter(b):
                out.extend(_expand_letter_range(a, b))
            else:
                out.extend(_expand_numeric_range(a, b))
        else:
            out.append(p)

    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
    seen = set()
    uniq = []
    for v in out:
        if v not in seen:
            seen.add(v)
            uniq.append(v)
    return uniq

# ===========================
# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –æ–±—â–∏—Ö —à–∞–±–ª–æ–Ω–æ–≤
# ===========================

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—É—á–∏—Ç—ã–≤–∞–µ–º –ø–∞–¥–µ–∂–∏ + –∑–∞—â–∏—â–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –æ—Ç –≤—Ö–æ–∂–¥–µ–Ω–∏–π –≤–Ω—É—Ç—Ä—å —Å–ª–æ–≤)
KW_ART  = r"(?:—Å—Ç(?:–∞—Ç—å—è|–∞—Ç—å–∏|–∞—Ç—å–µ|–∞—Ç—å—é|\.?)\w*)"
KW_PNT  = r"(?:–ø—É–Ω–∫—Ç\w*|(?<![–ê-–Ø–∞-—è—ë])–ø\.)"
KW_PART = r"(?:—á–∞—Å—Ç\w*|(?<![–ê-–Ø–∞-—è—ë])—á\.)"  
KW_SUBP = r"(?:–ø–æ–¥–ø—É–Ω–∫—Ç\w*|–ø–æ–¥–ø\.|–ø–ø\.)"

def compile_patterns(LAW_NAMED: str, LAW_NONCAP: str) -> Dict[str, re.Pattern]:
    PRE = r"(?:\b(?:–≤|–≤–æ|–Ω–∞|–∫|–∫–æ|–ø–æ|–æ–±|–æ–±–æ|–æ|–æ—Ç|—Å–æ|—Å|–¥–ª—è)\b\s*)?"
    TOK_ART    = fr"(?:{PRE}{KW_ART})"
    TOK_PNPART = fr"(?:{PRE}(?:{KW_PNT}|{KW_PART}))"
    TOK_SUBP   = fr"(?:{PRE}{KW_SUBP})"

    LA_AFTER_SUBP  = fr"(?=(?:\s*[,:;]?\s*(?:{TOK_PNPART}|{TOK_ART}))|[),.;]|$)"
    LA_AFTER_POINT = fr"(?=(?:\s*[,:;]?\s*(?:{TOK_ART}|{TOK_SUBP}))|[),.;]|$)"
    # –ë–ï–ó –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø:
    LA_AFTER_ART   = fr"(?=(?:\s*(?:{LAW_NONCAP})|\s*[,:;]?\s*(?:{TOK_PNPART}|{TOK_SUBP})|[),.;]|$))"

    patt_after = re.compile(
        fr"(?P<full>"
        fr"(?:{TOK_SUBP}\s*(?P<subp_vals>{VAL_CHUNK}?){LA_AFTER_SUBP}\s*[;,]?\s*)?"
        fr"(?:{TOK_PNPART}\s*(?P<point_vals>{VAL_CHUNK}?){LA_AFTER_POINT}\s*[;,]?\s*)?"
        fr"{TOK_ART}\s*(?P<article_vals>{VAL_CHUNK}?){LA_AFTER_ART}\s*"
        fr"(?P<law>{LAW_NAMED})"  
        fr")",
        flags=re.IGNORECASE
    )

    patt_before = re.compile(
        fr"(?P<full>"
        fr"(?P<law>{LAW_NAMED})\s*[,:;]?\s*"
        fr"{TOK_ART}\s*(?P<article_vals>{VAL_CHUNK}?){LA_AFTER_ART}"
        fr"(?:\s*[,:;]?\s*{TOK_PNPART}\s*(?P<point_vals>{VAL_CHUNK}?){LA_AFTER_POINT})?"
        fr"(?:\s*[,:;]?\s*{TOK_SUBP}\s*(?P<subp_vals>{VAL_CHUNK}?)(?=(?:\s*[),.;])|$))?"
        fr")",
        flags=re.IGNORECASE
    )

    patt_mid = re.compile(
        fr"(?P<full>"
        fr"{TOK_PNPART}\s*(?P<point_vals>{VAL_CHUNK}?){LA_AFTER_POINT}\s*[;,]?\s*"
        fr"{TOK_ART}\s*(?P<article_vals>{VAL_CHUNK}?){LA_AFTER_ART}\s*"
        fr"(?P<law>{LAW_NAMED})"
        fr")",
        flags=re.IGNORECASE
    )

    return {"after": patt_after, "before": patt_before, "mid": patt_mid}




def spans_overlap(a: Tuple[int, int], b: Tuple[int, int]) -> bool:
    # –ü–µ—Ä–µ–∫—Ä—ã–≤–∞—é—Ç—Å—è, –µ—Å–ª–∏ –æ–¥–∏–Ω –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é "–¥–æ" –¥—Ä—É–≥–æ–≥–æ
    return not (a[1] <= b[0] or b[1] <= a[0])


def prune_less_specific(items: List[dict]) -> List[dict]:
    """
    –£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ –±–µ–∑ subpoint, –µ—Å–ª–∏ –≤ —Ç–æ–º –∂–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ
    (–ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è —Å–ø–∞–Ω—ã) —É–∂–µ –µ—Å—Ç—å –∑–∞–ø–∏—Å–∏ —Å —Ç–µ–º –∂–µ (law_id, article, point)
    –∏ –ù–ï–ù–£–õ–ï–í–´–ú subpoint.
    """
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ (law_id, article, point)
    groups: Dict[Tuple[int, Optional[str], Optional[str]], List[dict]] = {}
    for it in items:
        key = (it["law_id"], it["article"], it["point"])
        groups.setdefault(key, []).append(it)

    to_drop = set()
    for key, arr in groups.items():
        with_sub = [x for x in arr if x["subpoint"] is not None]
        no_sub = [x for x in arr if x["subpoint"] is None]
        if not with_sub or not no_sub:
            continue
        # –µ—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Å–ø–∞–Ω–æ–≤ –º–µ–∂–¥—É –∑–∞–ø–∏—Å—è–º–∏ —Å subpoint –∏ –±–µ–∑ ‚Äî —É–¥–∞–ª—è–µ–º –±–µ–∑ subpoint
        for a in no_sub:
            for b in with_sub:
                if spans_overlap(a["span"], b["span"]):
                    to_drop.add(id(a))
                    break

    return [x for x in items if id(x) not in to_drop]



# ===========================
# –û—Å–Ω–æ–≤–Ω–æ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å
# ===========================

def extract_law_id_from_match(m: re.Match, lid_group_names: Dict[str, int]) -> Optional[int]:
    gd = m.groupdict()
    for name, lid in lid_group_names.items():
        if gd.get(name):
            return lid
    return None

from itertools import product
import re
from typing import Dict, List, Optional

_NUMERIC_POINT_RE = re.compile(r"^\d+(?:\.\d+)*$")
_SINGLE_LETTER_RE = re.compile(r"^[A-Za-z–ê-–Ø–∞-—è—ë]$")

def _is_numeric_point(value: Optional[str]) -> bool:
    """–ü—É–Ω–∫—Ç/—á–∞—Å—Ç—å –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º–∏ (—Ä–∞–∑—Ä–µ—à–∞–µ–º 3, 3.4, 10.1.2). None ‚Äî –æ–∫."""
    if value is None:
        return True
    return bool(_NUMERIC_POINT_RE.fullmatch(value))

def detect_links(
    text: str,
    codex_aliases: Dict[str, List[str]],
) -> List["ParsedRef"]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞.

    –ö–ª—é—á–µ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è:
      - –°—Ç–∞—Ç—å–∏ –ù–ï —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –ø–æ –¥–µ—Ñ–∏—Å–∞–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, '51.8-7' –æ—Å—Ç–∞—ë—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å),
        —á—Ç–æ–±—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ —Ç–µ—Å—Ç–∞–º–∏.
      - –ü—É–Ω–∫—Ç—ã –∏ –ø–æ–¥–ø—É–Ω–∫—Ç—ã —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º (–¥–∏–∞–ø–∞–∑–æ–Ω—ã/–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è).
      - –ë—É–∫–≤–µ–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –¥–æ–ø—É—Å—Ç–∏–º—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–ø. —Å').
    """
    # 1) –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —à–∞–±–ª–æ–Ω–æ–≤
    text_norm = normalize_text(text)
    _, LAW_NAMED, LAW_NONCAP, lid_group_names = build_alias_maps(codex_aliases)
    pats = compile_patterns(LAW_NAMED, LAW_NONCAP)

    # 2) –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏–∑ —Ä–∞–∑–Ω—ã—Ö ¬´–ø–æ—Ä—è–¥–∫–æ–≤¬ª
    matches: List[Tuple[int, int, re.Match]] = []
    for patt in pats.values():
        for m in patt.finditer(text_norm):
            matches.append((m.start(), m.end(), m))
    matches.sort(key=lambda t: (t[0], t[1]))

    # 3) –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Å—ã—Ä—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
    raw_items: List[Dict] = []
    for st, en, m in matches:
        law_id = extract_law_id_from_match(m, lid_group_names)
        if law_id is None:
            continue

        gd = m.groupdict()
        # –í–ê–ñ–ù–û: —Å—Ç–∞—Ç—å–∏ –ù–ï —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º (expand_hyphens=False),
        # –ø—É–Ω–∫—Ç—ã/–ø–æ–¥–ø—É–Ω–∫—Ç—ã ‚Äî —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º.
        art_vals = parse_values(gd.get("article_vals"), expand_hyphens=False)
        pnt_vals = parse_values(gd.get("point_vals"),   expand_hyphens=True)
        sub_vals = parse_values(gd.get("subp_vals"),    expand_hyphens=True)

        arts = art_vals or [None]
        pnts = pnt_vals or [None]
        subs = sub_vals or [None]

        for a, p, s in product(arts, pnts, subs):
            raw_items.append({
                "law_id": law_id,
                "article": a,
                "point": p,
                "subpoint": s,
                "span": (st, en),
            })

    # 4) –£–¥–∞–ª—è–µ–º –º–µ–Ω–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∑–∞–ø–∏—Å–∏ (–±–µ–∑ –ø–æ–¥–ø—É–Ω–∫—Ç–∞ —Ä—è–¥–æ–º —Å —Ç–µ–º–∏ –∂–µ law/art/point)
    raw_items = prune_less_specific(raw_items)

    # 5) –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞
    seen = set()
    result: List[ParsedRef] = []
    for it in raw_items:
        key = (it["law_id"], it["article"], it["point"], it["subpoint"])
        if key in seen:
            continue
        seen.add(key)
        result.append(ParsedRef(
            law_id=it["law_id"],
            article=it["article"],
            point=it["point"],
            subpoint=it["subpoint"],
        ))

    return result


# ============================
# FastAPI: lifecycle –∏ —ç–Ω–¥–ø–æ–π–Ω—Ç—ã
# ============================

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    try:
        law_path = Path(__file__).with_name("law_aliases.json")
        with law_path.open("r", encoding="utf-8") as f:
            codex_aliases = json.load(f)
        app.state.codex_aliases = codex_aliases
        logger.info("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∞–ª–∏–∞—Å—ã –∑–∞–∫–æ–Ω–æ–≤: %d –∑–∞–∫–æ–Ω–æ–≤", len(codex_aliases))
    except Exception as e:
        logger.exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å law_aliases.json: %s", e)
        raise

    # –ê–≤—Ç–æ—Å–∞–º–æ—Ç–µ—Å—Ç—ã (–±–µ–∑ HTTP), —á—Ç–æ–±—ã —Å—Ä–∞–∑—É —É–≤–∏–¥–µ—Ç—å, —á—Ç–æ –ø–∞—Ä—Å–µ—Ä –∂–∏–≤–æ–π
    if RUN_STARTUP_SELFTESTS:
        try:
            logger.info("üß™ –ó–∞–ø—É—Å–∫ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —Ç–µ—Å—Ç–æ–≤...")
            _run_self_tests(codex_aliases)
            logger.info("üß™ –¢–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã.")
        except Exception as e:
            logger.exception("–°–∞–º–æ—Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å —Å –æ—à–∏–±–∫–æ–π: %s", e)

    yield

    # Shutdown
    try:
        del app.state.codex_aliases
    except Exception:
        pass
    logger.info("üõë –°–µ—Ä–≤–∏—Å –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è...")


def get_codex_aliases(request: Request) -> Dict[str, List[str]]:
    return request.app.state.codex_aliases


app = FastAPI(
    title="Law Links Service",
    description="–°–µ—Ä–≤–∏—Å –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
    version="1.0.0",
    lifespan=lifespan,
)


@app.get("/health")
async def health_check():
    return {"status": "healthy"}


@app.post("/detect", response_model=LinksResponse)
async def detect_endpoint(
    data: TextRequest,
    codex_aliases: Dict[str, List[str]] = Depends(get_codex_aliases),
):
    text = data.text or ""
    try:
        refs = detect_links(text, codex_aliases)
        # –õ–æ–≥–∏—Ä—É–µ–º, –Ω–æ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ (–Ω–µ —Å–ø–∞–º–∏–º –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–∞—Ö)
        logger.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å—Å—ã–ª–æ–∫: %d", len(refs))
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –æ—Ç–≤–µ—Ç
        links = [
            LawLink(
                law_id=r.law_id,
                article=r.article if r.article is not None else None,
                point_article=r.point if r.point is not None else None,
                subpoint_article=r.subpoint if r.subpoint is not None else None,
            )
            for r in refs
        ]
        return LinksResponse(links=links)
    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ /detect: %s", e)
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 500, —á—Ç–æ–±—ã —è–≤–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å —Å–±–æ–π
        raise HTTPException(status_code=500, detail="Internal parsing error")


# ============================
# –¢–µ—Å—Ç—ã (–ª–æ–∫–∞–ª—å–Ω—ã–µ –∫–µ–π—Å—ã)
# ============================

def _run_self_tests(codex_aliases: Dict[str, List[str]]) -> None:
    """–ú–∏–Ω–∏-–Ω–∞–±–æ—Ä –±—ã—Å—Ç—Ä—ã—Ö smoke-—Ç–µ—Å—Ç–æ–≤ –ø–∞—Ä—Å–µ—Ä–∞."""
    tests = [
        # –û–∂–∏–¥–∞–µ—Ç—Å—è 3 —Å—Å—ã–ª–∫–∏ —Å subpoint 1/2/3
        ("–ø–ø. 1, 2 –∏ 3 –ø. 2 —Å—Ç. 3 –ù–ö –†–§", {"expect_count": 3,
                                           "expect_article": "3",
                                           "expect_point": "2",
                                           "expect_subpoints": {"1", "2", "3"}}),
        # –ó–∞–∫–æ–Ω –≤–Ω–∞—á–∞–ª–µ
        ("–£–ö –†–§, —Å—Ç. 145, –ø. 2, –ø–æ–¥–ø. –±", {"expect_count": 1,
                                           "expect_article": "145",
                                           "expect_point": "2",
                                           "expect_subpoints": {"–±"}}),
        # –î–µ—Å—è—Ç–∏—á–Ω–∞—è —Å—Ç–∞—Ç—å—è
        ("—á. 3, —Å—Ç. 30.1 –ö–æ–ê–ü –†–§", {"expect_count": 1,
                                    "expect_article": "30.1"}),
        # –î–∏–∞–ø–∞–∑–æ–Ω –≤ —Å—Ç–∞—Ç—å–µ —Å —É–∫–æ—Ä–æ—á–µ–Ω–Ω–æ–π –ø—Ä–∞–≤–æ–π –≥—Ä–∞–Ω–∏—Ü–µ–π
        ("—Å—Ç. 43.2-6 –ù–ö –†–§", {"expect_count": 1,"expect_article": "43.2-6"}),
        # –ü–æ–¥–ø—É–Ω–∫—Ç—ã —Å –±—É–∫–≤–∞–º–∏ –∏ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ–º –ø—É–Ω–∫—Ç–æ–≤
        ("–≤ –ø–æ–¥–ø—É–Ω–∫—Ç–∞—Ö –∞, –± –∏ –≤ –ø—É–Ω–∫—Ç–∞ 3.345, 23 –≤ —Å—Ç–∞—Ç—å–µ 66 –ù–ö –†–§",
         {"expect_min_count": 3, "expect_article": "66"}),
    ]

    for text, expectations in tests:
        refs = detect_links(text, codex_aliases)
        logger.debug("TEST: %s -> %s", text, refs)
        if "expect_count" in expectations:
            assert len(refs) == expectations["expect_count"], f"–û–∂–∏–¥–∞–ª–∏ {expectations['expect_count']}, –ø–æ–ª—É—á–∏–ª–∏ {len(refs)}"
        if "expect_min_count" in expectations:
            assert len(refs) >= expectations["expect_min_count"], f"–û–∂–∏–¥–∞–ª–∏ –º–∏–Ω–∏–º—É–º {expectations['expect_min_count']}, –ø–æ–ª—É—á–∏–ª–∏ {len(refs)}"
        if "expect_article" in expectations and refs:
            assert any(r.article == expectations["expect_article"] for r in refs), "–°—Ç–∞—Ç—å—è –Ω–µ —Å–æ–≤–ø–∞–ª–∞"
        if "expect_point" in expectations and refs:
            assert any(r.point == expectations["expect_point"] for r in refs), "–ü—É–Ω–∫—Ç –Ω–µ —Å–æ–≤–ø–∞–ª"
        if "expect_subpoints" in expectations and refs:
            got = {r.subpoint for r in refs if r.subpoint is not None}
            assert expectations["expect_subpoints"].issubset(got), f"–ü–æ–¥–ø—É–Ω–∫—Ç—ã –Ω–µ —Å–æ–≤–ø–∞–ª–∏: {got}"
        if "expect_articles" in expectations and refs:
            got = {r.article for r in refs if r.article is not None}
            assert expectations["expect_articles"] == got, f"–°—Ç–∞—Ç—å–∏ –Ω–µ —Å–æ–≤–ø–∞–ª–∏: {got}"

    logger.info("–°–∞–º–æ—Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã: %d –∫–µ–π—Å–æ–≤", len(tests))


# ============================
# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
# ============================

if __name__ == "__main__":
    logger.info("üöÄ –°–µ—Ä–≤–∏—Å –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    uvicorn.run(app, host="0.0.0.0", port=8978)
